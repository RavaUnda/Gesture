{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70f344a6",
      "metadata": {
        "id": "70f344a6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "import os\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import imageio\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ],
      "metadata": {
        "id": "-Y0QJcJsiLKq"
      },
      "id": "-Y0QJcJsiLKq",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access the dataset from my google drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm_6MZwpiPn1",
        "outputId": "eaba8d2c-f0fe-472d-bb56-614a76901bc1"
      },
      "id": "Rm_6MZwpiPn1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# added this piece later as it was throwing error - the backend to open the image files with iomode ri is not available.\n",
        "#!pip install imageio[pyav]\n"
      ],
      "metadata": {
        "id": "PY2oeQkiPPjB"
      },
      "id": "PY2oeQkiPPjB",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the file\n",
        "train_doc = np.random.permutation(open('/content/drive/MyDrive/Datasets/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/drive/MyDrive/Datasets/Project_data/val.csv').readlines())\n",
        "\n",
        "# multiple options are tried out\n",
        "batch_size = 8  # experiment with the batch size"
      ],
      "metadata": {
        "id": "s12AdMuTicyl"
      },
      "id": "s12AdMuTicyl",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optional code"
      ],
      "metadata": {
        "id": "AwpSgQ_Q7LY1"
      },
      "id": "AwpSgQ_Q7LY1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the generator\n",
        "def generator(source_path, folder_list, batch_size):\n",
        "    #print('Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = [11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28] # create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(source_path)//batch_size  # calculate the number of batches\n",
        "        ######\n",
        "        ### Remove above piece\n",
        "        ######\n",
        "\n",
        "        #num_batches = len(folder_list)//batch_size  # calculate the number of batches\n",
        "\n",
        "        for batch in range(num_batches):  # we iterate over the number of batches\n",
        "            # first try with x=20\n",
        "            batch_data = np.zeros((batch_size, 18, 100, 100, 3))  # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size, 5))  # batch_labels is the one hot representation of the output\n",
        "\n",
        "            for folder in range(batch_size):  # iterate over the batch_size\n",
        "                #print('Source path = ',source_path)\n",
        "                #print('\\n')\n",
        "                #print(\"Folder :\",folder)\n",
        "                #print(\"\\n\")\n",
        "                #print(\"files :\",t[folder + (batch*batch_size)].split(';')[0])\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])  # read all the images in the folder\n",
        "                for idx, item in enumerate(img_idx):  # Iterate over the frames/images of a folder to read them in\n",
        "                    image_path = os.path.join(source_path, t[folder + (batch * batch_size)].strip().split(';')[0], imgs[item])\n",
        "                    image = imageio.imread(image_path).astype(np.float32)\n",
        "\n",
        "                    # Resize the image\n",
        "                    image = cv2.resize(image, (100, 100))\n",
        "\n",
        "                    # Normalize and feed in the image\n",
        "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] / 255\n",
        "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] / 255\n",
        "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] / 255\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if (len(source_path) % batch_size) // 2 == 0:\n",
        "            batch_size = 2\n",
        "        else:\n",
        "            batch_size = 1\n",
        "        num_batches = len(source_path) % batch_size  # calculate the number of batches\n",
        "        for batch in range(num_batches):  # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size, 18, 100, 100, 3))  # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size, 5))  # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size):  # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0])  # read all the images in the folder\n",
        "                for idx, item in enumerate(img_idx):  # Iterate over the frames/images of a folder to read them in\n",
        "                    image_path = os.path.join(source_path, t[folder + (batch * batch_size)].strip().split(';')[0], imgs[item])\n",
        "                    image = imageio.imread(image_path).astype(np.float32)\n",
        "\n",
        "                    # Resize the image\n",
        "                    image = cv2.resize(image, (100, 100))\n",
        "\n",
        "                    # Normalize and feed in the image\n",
        "                    batch_data[folder, idx, :, :, 0] = image[:, :, 0] / 255\n",
        "                    batch_data[folder, idx, :, :, 1] = image[:, :, 1] / 255\n",
        "                    batch_data[folder, idx, :, :, 2] = image[:, :, 2] / 255\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels"
      ],
      "metadata": {
        "id": "BHH9DIXsilNb"
      },
      "id": "BHH9DIXsilNb",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = 'train'\n",
        "val_path = 'val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 25  # choose the number of epochs\n",
        "print('# epochs =', num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjioDGaSinYS",
        "outputId": "c1392828-3c69-4773-bef3-5b4a847d283a"
      },
      "id": "HjioDGaSinYS",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "Input_shape = (18, 100, 100, 3)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv3D(8, (3, 3, 3), padding='same', input_shape=Input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(16, (3, 3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv3D(64, (3, 3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "#model.add(Conv3D(64, (3, 3, 3)))\n",
        "#model.add(Activation('relu'))\n",
        "#model.add(BatchNormalization())\n",
        "#model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#model.add(Dense(64))\n",
        "#model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#softmax layer\n",
        "model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "LbaXJSS9iskb"
      },
      "id": "LbaXJSS9iskb",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.build(input_shape=(batch_size,) + Input_shape)\n",
        "\n",
        "#optimiser = optimizers.Adam(learning_rate=0.001)\n",
        "optimiser = optimizers.SGD(learning_rate=0.001, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjGN_KqqjNnI",
        "outputId": "c291e450-1deb-4005-9731-a295402ef54a"
      },
      "id": "wjGN_KqqjNnI",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 18, 100, 100, 8)   656       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 18, 100, 100, 8)   32        \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 18, 100, 100, 8)   0         \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3  (None, 9, 50, 50, 8)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 9, 50, 50, 16)     3472      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 9, 50, 50, 16)     64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 9, 50, 50, 16)     0         \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPoolin  (None, 4, 25, 25, 16)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 4, 25, 25, 32)     13856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 4, 25, 25, 32)     128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 4, 25, 25, 32)     0         \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPoolin  (None, 2, 12, 12, 32)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 2, 12, 12, 64)     55360     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 2, 12, 12, 64)     256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 2, 12, 12, 64)     0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 92165     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5)                 0         \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 165989 (648.39 KB)\n",
            "Trainable params: 165749 (647.46 KB)\n",
            "Non-trainable params: 240 (960.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generators\n",
        "train_generator = generator('/content/drive/MyDrive/Datasets/Project_data/train', train_doc, batch_size)\n",
        "val_generator = generator('/content/drive/MyDrive/Datasets/Project_data/val', val_doc, batch_size)"
      ],
      "metadata": {
        "id": "OrTHcqxXjXI3"
      },
      "id": "OrTHcqxXjXI3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ', '').replace(':', '_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "# filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=\"epoch\")\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1, mode=\"auto\", min_delta=1e-04, cooldown=1, min_lr=0)\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "metadata": {
        "id": "V84diB0YjoFM"
      },
      "id": "V84diB0YjoFM",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate steps per epoch and validation steps\n",
        "if (num_train_sequences % batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences / batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences // batch_size) + 1\n",
        "\n",
        "if (num_val_sequences % batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences / batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences // batch_size) + 1"
      ],
      "metadata": {
        "id": "SVHWtFKmj2ED"
      },
      "id": "SVHWtFKmj2ED",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oDk7Obr9j73f",
        "outputId": "8adb8925-10c8-4ca5-e3e7-47e9aa73ea7e"
      },
      "id": "oDk7Obr9j73f",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "83/83 [==============================] - ETA: 0s - loss: 1.6415 - categorical_accuracy: 0.1908 \n",
            "Epoch 1: saving model to model_init_2024-04-0318_37_42.639787/model-00001-1.64146-0.19079.h5\n",
            "83/83 [==============================] - 1680s 20s/step - loss: 1.6415 - categorical_accuracy: 0.1908 - val_loss: 1.6094 - val_categorical_accuracy: 0.1613 - lr: 0.0010\n",
            "Epoch 2/25\n",
            "66/83 [======================>.......] - ETA: 3:08 - loss: 1.6094 - categorical_accuracy: 0.2652"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-7-d48c4ab04f21>\", line 28, in generator\n    image = imageio.imread(image_path).astype(np.float32)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/__init__.py\", line 97, in imread\n    return imread_v2(uri, format=format, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/v2.py\", line 359, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py\", line 281, in imopen\n    raise err_type(err_msg)\n\nValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-7-d48c4ab04f21>\", line 28, in generator\n    image = imageio.imread(image_path).astype(np.float32)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/__init__.py\", line 97, in imread\n    return imread_v2(uri, format=format, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/v2.py\", line 359, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py\", line 281, in imopen\n    raise err_type(err_msg)\n\nValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1730]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2637e2d2a924>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2911\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m         )\n\u001b[0;32m-> 2913\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2914\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  ValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-7-d48c4ab04f21>\", line 28, in generator\n    image = imageio.imread(image_path).astype(np.float32)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/__init__.py\", line 97, in imread\n    return imread_v2(uri, format=format, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/v2.py\", line 359, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py\", line 281, in imopen\n    raise err_type(err_msg)\n\nValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_4]]\n  (1) INVALID_ARGUMENT:  ValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"<ipython-input-7-d48c4ab04f21>\", line 28, in generator\n    image = imageio.imread(image_path).astype(np.float32)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/__init__.py\", line 97, in imread\n    return imread_v2(uri, format=format, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/v2.py\", line 359, in imread\n    with imopen(uri, \"ri\", **imopen_args) as file:\n\n  File \"/usr/local/lib/python3.10/dist-packages/imageio/core/imopen.py\", line 281, in imopen\n    raise err_type(err_msg)\n\nValueError: Could not find a backend to open `/content/drive/MyDrive/Datasets/Project_data/train/WIN_20180907_16_05_58_Pro_Stop Gesture_new/WIN_20180907_16_05_58_Pro_00032.png`` with iomode `ri`.\nBased on the extension, the following plugins might add capable backends:\n  pyav:  pip install imageio[pyav]\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_1730]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}